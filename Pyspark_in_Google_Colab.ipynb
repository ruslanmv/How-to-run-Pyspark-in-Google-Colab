{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d0c9947",
      "metadata": {
        "id": "2d0c9947"
      },
      "source": [
        "#  How to install Pyspark in Google Colab, and join tables and more."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbfbe8e5",
      "metadata": {
        "id": "dbfbe8e5"
      },
      "source": [
        "Hello everyone, today we are going to discuss how to install Pyspark in Colab and explain\n",
        "how to use Broadcast  join more than 2 tables in PySpark."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660ebd61",
      "metadata": {
        "id": "660ebd61"
      },
      "source": [
        "When you are working with Big Data one of the most common task that you have is **merge different tables to a single table**.\n",
        "This happens when you have different sources and your company is asking your to have a single table in your Data Ware House **DWH**. For this project I will use Google Colab and I will explain how to install Pyspark and how to create different tables and merge them into a single table."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LhQlnDlfnEVQ"
      },
      "id": "LhQlnDlfnEVQ"
    },
    {
      "cell_type": "markdown",
      "id": "9305efa3",
      "metadata": {
        "id": "9305efa3"
      },
      "source": [
        "## Step 1. Mounting your Google Drive. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "955cf675",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "955cf675",
        "outputId": "e45d8f31-2c8d-4140-ee36-8e01e845bd1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e7f4825",
      "metadata": {
        "id": "5e7f4825"
      },
      "source": [
        "## Step 2. Installing Java\n",
        "Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b1ec65db",
      "metadata": {
        "id": "b1ec65db"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Installing Apache Spark"
      ],
      "metadata": {
        "id": "Z-fipXuSajbF"
      },
      "id": "Z-fipXuSajbF"
    },
    {
      "cell_type": "markdown",
      "id": "b0bbf791",
      "metadata": {
        "id": "b0bbf791"
      },
      "source": [
        "Next, we will install Apache Spark 3.3.0 with Hadoop 3 from [here](http://spark.apache.org/downloads.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a7766cb1",
      "metadata": {
        "id": "a7766cb1"
      },
      "outputs": [],
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "588ded18",
      "metadata": {
        "id": "588ded18"
      },
      "source": [
        "we list the files downloaded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWT-vjXpe5AJ",
        "outputId": "8e268209-2c90-4ce2-cf47-51c1a16d20e0"
      },
      "id": "wWT-vjXpe5AJ",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  spark-3.3.0-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we just need to unzip that folder."
      ],
      "metadata": {
        "id": "B1mP-ur2fK9Q"
      },
      "id": "B1mP-ur2fK9Q"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1d716db3",
      "metadata": {
        "id": "1d716db3"
      },
      "outputs": [],
      "source": [
        "!tar xf spark-3.3.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a24d4857",
      "metadata": {
        "id": "a24d4857"
      },
      "source": [
        "There is one last thing that we need to install and that is the [findspark](https://pypi.org/project/findspark/) library. It will locate Spark on the system and import it as a regular library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ce7b7722",
      "metadata": {
        "id": "ce7b7722"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e0ed92",
      "metadata": {
        "id": "d1e0ed92"
      },
      "source": [
        "This will enable us to run Pyspark in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "93c89eeb",
      "metadata": {
        "id": "93c89eeb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021e0aa0",
      "metadata": {
        "id": "021e0aa0"
      },
      "source": [
        "We need to locate Spark in the system. For that, we import findspark and use the findspark.init() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d7f25dda",
      "metadata": {
        "id": "d7f25dda"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f90bff",
      "metadata": {
        "id": "64f90bff"
      },
      "source": [
        "Now, we can import [SparkSession](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.SparkSession) from [pyspark.sql](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark-sql-module) and create a SparkSession, which is the entry point to Spark.\n",
        "\n",
        "You can give a name to the session using appName() and add some configurations with config() if you wish."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dfc30f",
      "metadata": {
        "id": "e0dfc30f"
      },
      "source": [
        "# Data Science with Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3E-RhUvv4_Dz",
      "metadata": {
        "id": "3E-RhUvv4_Dz"
      },
      "source": [
        "Due to we are going \n",
        "Run the code below to download the compressed datasets data.zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3oGHH9bv4mNE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oGHH9bv4mNE",
        "outputId": "dd0b840f-8df5-45ce-8e2a-d3f14c62aadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-25 13:22:11--  https://github.com/ruslanmv/Data-Science-in-Jupyter-Notebook/raw/master/data.zip\n",
            "Resolving github.com (github.com)... 52.69.186.44\n",
            "Connecting to github.com (github.com)|52.69.186.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ruslanmv/Data-Science-in-Jupyter-Notebook/master/data.zip [following]\n",
            "--2022-06-25 13:22:11--  https://raw.githubusercontent.com/ruslanmv/Data-Science-in-Jupyter-Notebook/master/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28337418 (27M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  27.02M   146MB/s    in 0.2s    \n",
            "\n",
            "2022-06-25 13:22:12 (146 MB/s) - ‘data.zip’ saved [28337418/28337418]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/ruslanmv/Data-Science-in-Jupyter-Notebook/raw/master/data.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tJ1QzmgM5HZD",
      "metadata": {
        "id": "tJ1QzmgM5HZD"
      },
      "source": [
        "You can then unzip the archive using the zipfile module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9U78MrwP5H5h",
      "metadata": {
        "id": "9U78MrwP5H5h"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "# Unzip the dataset\n",
        "local_zip = './data.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./')\n",
        "zip_ref.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9c88d5",
      "metadata": {
        "id": "cd9c88d5"
      },
      "source": [
        "**What is PySpark?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f901c7b",
      "metadata": {
        "id": "3f901c7b"
      },
      "source": [
        "PySpark is an Apache Spark interface in Python. It is used for\n",
        "collaborating with Spark using APIs written in Python. It also supports\n",
        "Spark’s features like Spark DataFrame, Spark SQL, Spark Streaming, Spark\n",
        "MLlib and Spark Core."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0ccb32",
      "metadata": {
        "id": "ed0ccb32"
      },
      "source": [
        "**What is PySpark SparkContext?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3e18677",
      "metadata": {
        "id": "c3e18677"
      },
      "source": [
        "PySpark SparkContext is an initial entry point of the spark\n",
        "functionality. It also represents Spark Cluster Connection and can be\n",
        "used for creating the Spark RDDs (Resilient Distributed Datasets) and\n",
        "broadcasting the variables on the cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a55cd14",
      "metadata": {
        "id": "3a55cd14"
      },
      "source": [
        "Now, we can import [SparkSession](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.SparkSession) from [pyspark.sql](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark-sql-module) and create a SparkSession, which is the entry point to Spark.\n",
        "\n",
        "You can give a name to the session using appName() and add some configurations with config() if you wish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "db66d241",
      "metadata": {
        "id": "db66d241"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, print the SparkSession variable."
      ],
      "metadata": {
        "id": "xpA6j7Irf_fm"
      },
      "id": "xpA6j7Irf_fm"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2be043b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "2be043b0",
        "outputId": "dc8923f4-a08c-48e5-f168-a1610ad38d67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fbec2326110>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5e0d22a947ae:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before enter in more details with Pyspark let us Merge two DataFrames in PySpark"
      ],
      "metadata": {
        "id": "NmOlta_tgqVm"
      },
      "id": "NmOlta_tgqVm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge two DataFrames in PySpark"
      ],
      "metadata": {
        "id": "FeToYa8ShdQb"
      },
      "id": "FeToYa8ShdQb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark union() and unionAll() transformations are used to merge two or more DataFrame’s of the same schema or structure.\n",
        "\n",
        "First, let’s create two DataFrame with the same schema."
      ],
      "metadata": {
        "id": "LV1scUC7ibY1"
      },
      "id": "LV1scUC7ibY1"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('App').getOrCreate()\n",
        "\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
        "  ]\n",
        "\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtRisDieiifV",
        "outputId": "21ac6740-94ea-435f-a16d-da0fa4efce6b"
      },
      "id": "UtRisDieiifV",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second DataFrame"
      ],
      "metadata": {
        "id": "XVQWjC1-i4yI"
      },
      "id": "XVQWjC1-i4yI"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
        "  ]\n",
        "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
        "\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZssTQEni7WS",
        "outputId": "6938fbbd-e08c-49ff-f0e9-53c9d1fdde16"
      },
      "id": "3ZssTQEni7WS",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bylGUc5ciCiG"
      },
      "id": "bylGUc5ciCiG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge two or more DataFrames using union"
      ],
      "metadata": {
        "id": "S9NtNgaPjANt"
      },
      "id": "S9NtNgaPjANt"
    },
    {
      "cell_type": "code",
      "source": [
        "unionDF = df.union(df2)\n",
        "unionDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmkb5IkDjDkc",
        "outputId": "1719a355-9f56-4f87-ef07-0083f51c0a7f"
      },
      "id": "gmkb5IkDjDkc",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create dataframes with columns ‘a’ and ‘b’ of some random values and pass all these three dataframe to our above-created method unionAll() and get the resultant dataframe as output and show the result."
      ],
      "metadata": {
        "id": "rtFMrPcvjKM2"
      },
      "id": "rtFMrPcvjKM2"
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules\n",
        "from pyspark.sql import SparkSession\n",
        "import functools\n",
        "\n",
        "# explicit function\n",
        "def unionAll(dfs):\n",
        "\treturn functools.reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n",
        "\n",
        "\n",
        "df1 = spark.createDataFrame([[1, 1], [2, 2]], ['a', 'b'])\n",
        "\n",
        "# different column order.\n",
        "df2 = spark.createDataFrame([[3, 333], [4, 444]], ['b', 'a'])\n",
        "df3 = spark.createDataFrame([[555, 5], [666, 6]], ['b', 'a'])\n",
        "\n",
        "unioned_df = unionAll([df1, df2, df3])\n",
        "unioned_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nQzMsYehfxt",
        "outputId": "7fefdc79-59b4-4e96-acbc-18d1e5d5d21d"
      },
      "id": "1nQzMsYehfxt",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|  a|  b|\n",
            "+---+---+\n",
            "|  1|  1|\n",
            "|  2|  2|\n",
            "|333|  3|\n",
            "|444|  4|\n",
            "|  5|555|\n",
            "|  6|666|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Broadcast Join"
      ],
      "metadata": {
        "id": "nmk2DXFwjcr0"
      },
      "id": "nmk2DXFwjcr0"
    },
    {
      "cell_type": "markdown",
      "source": [
        " This join can be used for the data frame that is smaller in size which can be broadcasted with the PySpark application to be used further.\n",
        "\n",
        "It is a join operation of a large data frame with a smaller data frame in PySpark Join model. It reduces the data shuffling by broadcasting the smaller data frame in the nodes of PySpark cluster"
      ],
      "metadata": {
        "id": "G3GThIZujqxv"
      },
      "id": "G3GThIZujqxv"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast"
      ],
      "metadata": {
        "id": "JnI7xxBxl_P-"
      },
      "id": "JnI7xxBxl_P-",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('App').getOrCreate()\n",
        "sc=spark.sparkContext"
      ],
      "metadata": {
        "id": "eBIpvMJqkXiv"
      },
      "id": "eBIpvMJqkXiv",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [{'Name':'Jhon','ID':21.528,'Add':'USA'},{'Name':'Joe','ID':3.69,'Add':'USA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USA'},{'Name':'Joe','ID':5.33,'Add':'INA'}]\n",
        "a = sc.parallelize(data1)"
      ],
      "metadata": {
        "id": "_aleg6v0jrg-"
      },
      "id": "_aleg6v0jrg-",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD is created using sc.parallelize."
      ],
      "metadata": {
        "id": "Ls40-O53j6mK"
      },
      "id": "Ls40-O53j6mK"
    },
    {
      "cell_type": "code",
      "source": [
        "b = spark.createDataFrame(a)\n",
        "b.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_evZbrjj9va",
        "outputId": "84aab262-c601-43a5-c599-d01f8a86e0f8"
      },
      "id": "x_evZbrjj9va",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----+\n",
            "|Add|    ID|Name|\n",
            "+---+------+----+\n",
            "|USA|21.528|Jhon|\n",
            "|USA|  3.69| Joe|\n",
            "|IND|  2.48|Tina|\n",
            "|USA| 22.22|Jhon|\n",
            "|INA|  5.33| Joe|\n",
            "+---+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create the other data frame with data2. "
      ],
      "metadata": {
        "id": "_vONrU3_lbeH"
      },
      "id": "_vONrU3_lbeH"
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = [{'Name':'Jhon','ID':21.528,'Add':'USA'},{'Name':'Joe','ID':3.69,'Add':'USeA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USdA'},{'Name':'Joe','ID':5.33,'Add':'rsa'}]"
      ],
      "metadata": {
        "id": "prXfMgC6lfJq"
      },
      "id": "prXfMgC6lfJq",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = sc.parallelize(data2)"
      ],
      "metadata": {
        "id": "w20zFEvdlaaY"
      },
      "id": "w20zFEvdlaaY",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = spark.createDataFrame(c)"
      ],
      "metadata": {
        "id": "rPAzvbmWluMB"
      },
      "id": "rPAzvbmWluMB",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to broadcast the data in the data frame, the method broadcast is used to broadcast the data frame out of it."
      ],
      "metadata": {
        "id": "8SGJNpKtj7j0"
      },
      "id": "8SGJNpKtj7j0"
    },
    {
      "cell_type": "code",
      "source": [
        "e = broadcast(b)"
      ],
      "metadata": {
        "id": "-zgts5j3lxsT"
      },
      "id": "-zgts5j3lxsT",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now join both the data frame using a particular column name out of it. This avoids the data shuffling throughout the network in PySpark application."
      ],
      "metadata": {
        "id": "zaST0gMBmKAW"
      },
      "id": "zaST0gMBmKAW"
    },
    {
      "cell_type": "code",
      "source": [
        "f = d.join(broadcast(e),d.Add == e.Add)"
      ],
      "metadata": {
        "id": "nMtxuEzgmMWZ"
      },
      "id": "nMtxuEzgmMWZ",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK4wrSVamRht",
        "outputId": "87e288e7-cbd1-4b0f-ecb6-cab6a92e2e82"
      },
      "id": "uK4wrSVamRht",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----+---+------+----+\n",
            "|Add|    ID|Name|Add|    ID|Name|\n",
            "+---+------+----+---+------+----+\n",
            "|USA|21.528|Jhon|USA| 22.22|Jhon|\n",
            "|USA|21.528|Jhon|USA|  3.69| Joe|\n",
            "|USA|21.528|Jhon|USA|21.528|Jhon|\n",
            "|IND|  2.48|Tina|IND|  2.48|Tina|\n",
            "+---+------+----+---+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also do the join operation over the other columns also that can be further used for the creation of a new data frame."
      ],
      "metadata": {
        "id": "kfxkExlhmZak"
      },
      "id": "kfxkExlhmZak"
    },
    {
      "cell_type": "code",
      "source": [
        "f = d.join(broadcast(e),d.Name == e.Name)\n",
        "f.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_avHV1sFmbRz",
        "outputId": "c575b706-531f-4e19-f0c1-5b5a8e3a93a0"
      },
      "id": "_avHV1sFmbRz",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+----+---+------+----+\n",
            "| Add|    ID|Name|Add|    ID|Name|\n",
            "+----+------+----+---+------+----+\n",
            "| USA|21.528|Jhon|USA| 22.22|Jhon|\n",
            "| USA|21.528|Jhon|USA|21.528|Jhon|\n",
            "|USeA|  3.69| Joe|INA|  5.33| Joe|\n",
            "|USeA|  3.69| Joe|USA|  3.69| Joe|\n",
            "| IND|  2.48|Tina|IND|  2.48|Tina|\n",
            "|USdA| 22.22|Jhon|USA| 22.22|Jhon|\n",
            "|USdA| 22.22|Jhon|USA|21.528|Jhon|\n",
            "| rsa|  5.33| Joe|INA|  5.33| Joe|\n",
            "| rsa|  5.33| Joe|USA|  3.69| Joe|\n",
            "+----+------+----+---+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Export a table dataframe in PySpark to csv"
      ],
      "metadata": {
        "id": "fE3gxodSodM5"
      },
      "id": "fE3gxodSodM5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0HG65oGioaoj"
      },
      "id": "0HG65oGioaoj"
    },
    {
      "cell_type": "code",
      "source": [
        "f.toPandas().to_csv('data.csv')"
      ],
      "metadata": {
        "id": "FxuI0zgCojwF"
      },
      "id": "FxuI0zgCojwF",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data into PySpark"
      ],
      "metadata": {
        "id": "Joja7M4Cn7MH"
      },
      "id": "Joja7M4Cn7MH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to load the dataset. We will use the [read.csv](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.DataFrameReader.csv) module. The **inferSchema** parameter provided will enable Spark to automatically determine the data type for each column but it has to go over the data once. If you don’t want that to happen, then you can instead provide the schema explicitly in the **schema** parameter."
      ],
      "metadata": {
        "id": "71ddRd8an90Y"
      },
      "id": "71ddRd8an90Y"
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "dgIP5wFHoKac"
      },
      "id": "dgIP5wFHoKac",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s get started with PySpark in more details!"
      ],
      "metadata": {
        "id": "yYzPgQMlmkxJ"
      },
      "id": "yYzPgQMlmkxJ"
    },
    {
      "cell_type": "markdown",
      "id": "6aada87b",
      "metadata": {
        "id": "6aada87b"
      },
      "source": [
        "**What is spark-submit?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732f69c3",
      "metadata": {
        "id": "732f69c3"
      },
      "source": [
        "Spark-submit is a utility to run a pyspark application job by specifying\n",
        "options and configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c8672b",
      "metadata": {
        "id": "65c8672b"
      },
      "outputs": [],
      "source": [
        "spark-submit \\\n",
        "     - -master < master-url > \\\n",
        "    --deploy-mode < deploy-mode > \\\n",
        "    --conf < key <= <value > \\\n",
        "    --driver-memory < value > g \\\n",
        "    - -executor-memory < value > g \\\n",
        "    - -executor-cores < number of cores > \\\n",
        "    --jars < comma separated dependencies > \\\n",
        "    --packages < package name > \\\n",
        "      --py-files \\\n",
        "    < application > <application args >\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29129dfd",
      "metadata": {
        "id": "29129dfd"
      },
      "source": [
        "where"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c1010b",
      "metadata": {
        "id": "50c1010b"
      },
      "source": [
        "–master : Cluster Manager (yarn, mesos, Kubernetes, local, local(k))  \n",
        "–deploy-mode: Either cluster or client  \n",
        "–conf: We can provide runtime configurations, shuffle parameters,\n",
        "application configurations using –conf. Ex: –conf\n",
        "spark.sql.shuffle.partitions = 300  \n",
        "–driver-memory : Amount of memory to allocate for a driver (Default:\n",
        "1024M).  \n",
        "–executor-memory : Amount of memory to use for the executor process.  \n",
        "–executor cores : Number of CPU cores to use for the executor process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d6ebad",
      "metadata": {
        "id": "29d6ebad"
      },
      "source": [
        "**What are RDDs in PySpark?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f248e972",
      "metadata": {
        "id": "f248e972"
      },
      "source": [
        "RDDs expand to Resilient Distributed Datasets. These are the elements\n",
        "that are used for running and operating on multiple nodes to perform\n",
        "parallel processing on a cluster. Since RDDs are suited for parallel\n",
        "processing, they are immutable elements. This means that once we create\n",
        "RDD, we cannot modify it. RDDs are also fault-tolerant which means that\n",
        "whenever failure happens, they can be recovered automatically. Multiple\n",
        "operations can be performed on RDDs to perform a certain task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "603ed06b",
      "metadata": {
        "id": "603ed06b"
      },
      "source": [
        "-   Data Representation: RDD is a distributed collection of data\n",
        "    elements without any schema\n",
        "-   Optimization: No in-built optimization engine for RDDs\n",
        "-   Schema: we need to define the schema manually.\n",
        "-   Aggregation Operation: RDD is slower than both Dataframes and\n",
        "    Datasets to perform simple operations like grouping the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff25315e",
      "metadata": {
        "id": "ff25315e"
      },
      "source": [
        "**Creation of RDD using textFile API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2179c69",
      "metadata": {
        "id": "a2179c69"
      },
      "outputs": [],
      "source": [
        "rdd = spark.sparkContext.textFile('practice/test')\n",
        "rdd.take(5)\n",
        "for i in rdd.take(5):\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52bf463c",
      "metadata": {
        "id": "52bf463c"
      },
      "source": [
        "**Get the Number of Partitions in the RDD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e98f411",
      "metadata": {
        "id": "3e98f411"
      },
      "outputs": [],
      "source": [
        "rdd.getNumPartitions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7cf7b39",
      "metadata": {
        "id": "e7cf7b39"
      },
      "source": [
        "**Get the Number of elements in each partition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9964ecf6",
      "metadata": {
        "id": "9964ecf6"
      },
      "outputs": [],
      "source": [
        "rdd.glom().map(len).collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0270db6b",
      "metadata": {
        "id": "0270db6b"
      },
      "source": [
        "**Create RDD using textFile API and a defined number of partitions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59a3efab",
      "metadata": {
        "id": "59a3efab"
      },
      "outputs": [],
      "source": [
        "rdd = spark.sparkContext.textFile('practice/test', 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd93397",
      "metadata": {
        "id": "6cd93397"
      },
      "source": [
        "**Create a RDD from a Python List**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "425306d9",
      "metadata": {
        "id": "425306d9"
      },
      "outputs": [],
      "source": [
        "lst = [1, 2, 3, 4, 5, 6, 7]\n",
        "rdd = spark.sparkContext.parallelize(lst)\n",
        "for i in rdd.take(5):\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bad5333",
      "metadata": {
        "id": "0bad5333"
      },
      "source": [
        "**Create a RDD from a Python List**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55e63034",
      "metadata": {
        "id": "55e63034"
      },
      "outputs": [],
      "source": [
        "lst = [1, 2, 3, 4, 5, 6, 7]\n",
        "rdd = spark.sparkContext.parallelize(lst)\n",
        "for i in rdd.take(5):\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32805805",
      "metadata": {
        "id": "32805805"
      },
      "source": [
        "**Create a RDD from local file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fefa535",
      "metadata": {
        "id": "8fefa535"
      },
      "outputs": [],
      "source": [
        "lst = open('/staging/test/sample.txt').read().splitlines()\n",
        "lst[0:10]\n",
        "rdd = spark.sparkContext.parallelize(lst)\n",
        "for i in rdd.take(5):\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42522098",
      "metadata": {
        "id": "42522098"
      },
      "source": [
        "**Create RDD from range function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e267fb",
      "metadata": {
        "id": "08e267fb"
      },
      "outputs": [],
      "source": [
        "lst1 = range(10)\n",
        "rdd = spark.sparkContext.parallelize(lst1)\n",
        "for i in rdd.take(5):\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f13f3a",
      "metadata": {
        "id": "95f13f3a"
      },
      "source": [
        "**Create RDD from a DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd5f6fa3",
      "metadata": {
        "id": "bd5f6fa3"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame(\n",
        "    data=(('robert', 35), ('Mike', 45)), schema=('name', 'age'))\n",
        "df.printSchema()\n",
        "df.show()\n",
        "rdd1 = df.rdd\n",
        "type(rdd1)\n",
        "for i in rdd1.take(2):\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0354b124",
      "metadata": {
        "id": "0354b124"
      },
      "source": [
        "**What are Dataframes?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce5a8a0",
      "metadata": {
        "id": "7ce5a8a0"
      },
      "source": [
        "It was introduced first in Spark version 1.3 to overcome the limitations\n",
        "of the Spark RDD. Spark Dataframes are the distributed collection of the\n",
        "data points, but here, the data is organized into the named columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa00d323",
      "metadata": {
        "id": "fa00d323"
      },
      "source": [
        "-   Data Representation:It is also the distributed collection organized\n",
        "    into the named columns\n",
        "-   Optimization: It uses a catalyst optimizer for optimization.\n",
        "-   Schema: It will automatically find out the schema of the dataset.\n",
        "-   Aggregation Operation: It performs aggregation faster than both RDDs\n",
        "    and Datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8d6fd5d",
      "metadata": {
        "id": "b8d6fd5d"
      },
      "source": [
        "**What are Datasets?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec984c5",
      "metadata": {
        "id": "dec984c5"
      },
      "source": [
        "Spark Datasets is an extension of Dataframes API with the benefits of\n",
        "both RDDs and the Datasets. It is fast as well as provides a type-safe\n",
        "interface."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1434c8",
      "metadata": {
        "id": "4d1434c8"
      },
      "source": [
        "-   Data Representation:It is an extension of Dataframes with more\n",
        "    features like type-safety and object-oriented interface.\n",
        "-   Optimization:It uses a catalyst optimizer for optimization.\n",
        "-   Schema: It will automatically find out the schema of the dataset.\n",
        "-   Aggregation Operation:Dataset is faster than RDDs but a bit slower\n",
        "    than Dataframes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf4b0ab",
      "metadata": {
        "id": "5cf4b0ab"
      },
      "source": [
        "**What type of operation has Pyspark?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "198e5bff",
      "metadata": {
        "id": "198e5bff"
      },
      "source": [
        "The operations can be of 2 types, actions and transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e8f0cf9",
      "metadata": {
        "id": "4e8f0cf9"
      },
      "source": [
        "**What is Transformation in Pyspark?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a03e7ce",
      "metadata": {
        "id": "0a03e7ce"
      },
      "source": [
        "Transformation: These operations when applied on RDDs result in the\n",
        "creation of a new RDD. Some of the examples of transformation operations\n",
        "are filter, groupBy, map. Let us take an example to demonstrate\n",
        "transformation operation by considering filter() operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be5285c0",
      "metadata": {
        "id": "be5285c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"Transdormation Demo\")\n",
        "words_list = sc.parallelize(\n",
        "    [\"pyspark\",\n",
        "     \"interview\",\n",
        "     \"questions\"]\n",
        ")\n",
        "filtered_words = words_list.filter(lambda x: 'interview' in x)\n",
        "filtered = filtered_words.collect()\n",
        "print(filtered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8414483",
      "metadata": {
        "id": "d8414483"
      },
      "outputs": [],
      "source": [
        "\n",
        "The output of the above code would be:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f264dddc",
      "metadata": {
        "id": "f264dddc"
      },
      "outputs": [],
      "source": [
        "[\n",
        "    \"interview\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db787bb",
      "metadata": {
        "id": "4db787bb"
      },
      "source": [
        "**What is Action in Pyspark?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c4ed45",
      "metadata": {
        "id": "d4c4ed45"
      },
      "source": [
        "Action: These operations instruct Spark to perform some computations on\n",
        "the RDD and return the result to the driver. It sends data from the\n",
        "Executer to the driver. count(), collect(), take() are some of the\n",
        "examples. Let us consider an example to demonstrate action operation by\n",
        "making use of the count() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21dab4ea",
      "metadata": {
        "id": "21dab4ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"Action Demo\")\n",
        "words = sc.parallelize(\n",
        "    [\"pyspark\",\n",
        "     \"interview\",\n",
        "     \"questions\"]\n",
        ")\n",
        "counts = words.count()\n",
        "print(\"Count of elements in RDD -> \",  counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16523dec",
      "metadata": {
        "id": "16523dec"
      },
      "outputs": [],
      "source": [
        "\n",
        "we count the number of elements in the spark RDDs. The output of this\n",
        "code is Count of elements in RDD -\\> 3 \\  # \\# Creating DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9dca3e",
      "metadata": {
        "id": "bc9dca3e"
      },
      "source": [
        "**From RDDs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e0cf1f",
      "metadata": {
        "id": "90e0cf1f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ba57a4",
      "metadata": {
        "id": "61ba57a4"
      },
      "source": [
        "**Infer Schema**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f509b0f8",
      "metadata": {
        "id": "f509b0f8"
      },
      "outputs": [],
      "source": [
        "sc = spark.sparkContext\n",
        "lines = sc.textFile(\"people.txt\")\n",
        "parts = lines.map(lambda l: l.split(\",\"))\n",
        "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
        "peopledf = spark.createDataFrame(people)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5b77a2",
      "metadata": {
        "id": "0d5b77a2"
      },
      "source": [
        "\n",
        "**Specify Schema**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ffdf5a",
      "metadata": {
        "id": "86ffdf5a"
      },
      "outputs": [],
      "source": [
        "\n",
        "people = parts.map(lambda p: Row(name=p[0],\n",
        "                                 age=int(p[1].strip())))\n",
        "schemaString = \"name age\"\n",
        "fields = [StructField(field_name, StringType(), True) for\n",
        "          field_name in schemaString.split()]\n",
        "schema = StructType(fields)\n",
        "spark.createDataFrame(people, schema).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53045f3",
      "metadata": {
        "id": "c53045f3"
      },
      "source": [
        "**From Spark Data Sources**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2340ada9",
      "metadata": {
        "id": "2340ada9"
      },
      "source": [
        "**JSON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97aa6089",
      "metadata": {
        "id": "97aa6089"
      },
      "outputs": [],
      "source": [
        "df = spark.read.json(\"customer.json\")\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ba0fff",
      "metadata": {
        "id": "64ba0fff"
      },
      "outputs": [],
      "source": [
        "\n",
        "df2 = spark.read.load(\"people.json\", format=\"json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452c1bfe",
      "metadata": {
        "id": "452c1bfe"
      },
      "source": [
        "\n",
        "**Parquet files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08dfa6ee",
      "metadata": {
        "id": "08dfa6ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "df3 = spark.read.load(\"people.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c552af6",
      "metadata": {
        "id": "0c552af6"
      },
      "source": [
        "\n",
        "**TXT files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db330d3",
      "metadata": {
        "id": "5db330d3"
      },
      "outputs": [],
      "source": [
        "df4 = spark.read.text(\"people.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589b695f",
      "metadata": {
        "id": "589b695f"
      },
      "source": [
        "### Filter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984a5c5b",
      "metadata": {
        "id": "984a5c5b"
      },
      "source": [
        "Filter entries of age, only keep those records of which the values are\n",
        "\\>24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fd7f22",
      "metadata": {
        "id": "91fd7f22"
      },
      "outputs": [],
      "source": [
        "df.filter(df[\"age\"] > 24).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56523b30",
      "metadata": {
        "id": "56523b30"
      },
      "source": [
        "Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38fc8f30",
      "metadata": {
        "id": "38fc8f30"
      },
      "outputs": [],
      "source": [
        "df = df.dropDuplicates()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a8b04c",
      "metadata": {
        "id": "d2a8b04c"
      },
      "source": [
        "### Queries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6835943f",
      "metadata": {
        "id": "6835943f"
      },
      "source": [
        "**What is PySpark SQL?** PySpark SQL is the most popular PySpark module\n",
        "that is used to process structured columnar data. Once a DataFrame is\n",
        "created, we can interact with data using the SQL syntax. Spark SQL is\n",
        "used for bringing native raw SQL queries on Spark by using select,\n",
        "where, group by, join, union etc. For using PySpark SQL, the first step\n",
        "is to create a temporary table on DataFrame by using\n",
        "createOrReplaceTempView()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e63901e",
      "metadata": {
        "id": "9e63901e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e667af5f",
      "metadata": {
        "id": "e667af5f"
      },
      "source": [
        "**Select**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd9aa68",
      "metadata": {
        "id": "ffd9aa68"
      },
      "outputs": [],
      "source": [
        "\n",
        " df.select(\"firstName\").show()  # Show all entries in firstNome column\n",
        " df.select(\"firstName\", \"lastName\") \\\n",
        "       .show()\n",
        " df.select(\"firstName\",  # Show all entries in firstNome, age and type\n",
        "                \"age\",\n",
        "                explode(''phoneNumber'') \\\n",
        "                .alias(''contactlnfo')') \\\n",
        "        .select(\"contactlnfo.type\",\n",
        "                \"firstName\",\n",
        "                \"age\") \\\n",
        "        .show()\n",
        " df.select(df[\"firstName\", df[\"age\"] + 1)  # Show all entries in firstName and age,\n",
        "       .show()  # add 1 to the entries of age\n",
        " df.select(df['age'] > 24).show()  # Show all entries where age >24\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249b6c05",
      "metadata": {
        "id": "249b6c05"
      },
      "source": [
        "\n",
        "**When**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167b3052",
      "metadata": {
        "id": "167b3052"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.select(\"firstName\",  # Show firstName and 0 or 1 depending on age >30\n",
        "          F.when(df.age > 30, 1) \\\n",
        "          .otherwise(0)) \\\n",
        "    .show()\n",
        "# Show firstName if in the given options\n",
        "df[df.firstName.isin(\"Jane\", \"Boris\")]\n",
        ".collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a836e25e",
      "metadata": {
        "id": "a836e25e"
      },
      "source": [
        "\n",
        "**Like**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ea1837",
      "metadata": {
        "id": "c6ea1837"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Show firstName, and lastName is TRUE if lastName is like Smith\n",
        " df.select(\"firstName\",\n",
        "                df.lastName .like(''Smith')')\n",
        "      .show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e6e1dd",
      "metadata": {
        "id": "44e6e1dd"
      },
      "source": [
        "\n",
        "**Startswith** - **Endswith**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9581a34d",
      "metadata": {
        "id": "9581a34d"
      },
      "outputs": [],
      "source": [
        "df.select(\"firstName\",  # Show firstName, and TRUE if lastName starts with Sm\n",
        "          df.lastName \\\n",
        "          .startswith(\"Sm\")) \\\n",
        "    .show()\n",
        "df.select(df.lastName.endswith(\"th\"))\\  # Show last names ending in th\n",
        ".show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f91c72",
      "metadata": {
        "id": "75f91c72"
      },
      "source": [
        "**Substring**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec486bd",
      "metadata": {
        "id": "0ec486bd"
      },
      "outputs": [],
      "source": [
        "     df.select(df.firstName.substr(l, 3) \\  # Return substrings of firstName\n",
        "          .alias(''name')') \\\n",
        "          .collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0579e00f",
      "metadata": {
        "id": "0579e00f"
      },
      "source": [
        "**Between**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20a8b83c",
      "metadata": {
        "id": "20a8b83c"
      },
      "outputs": [],
      "source": [
        "# Show age: values are TRUE if between 22 and 24\n",
        "df.select(df.age.between(22, 2s)) \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df12a010",
      "metadata": {
        "id": "df12a010"
      },
      "source": [
        "### Add, Update & Remove Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce554af",
      "metadata": {
        "id": "dce554af"
      },
      "source": [
        "**Adding Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c5c691",
      "metadata": {
        "id": "44c5c691"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn('city', df.address.city) \\\n",
        "    .withColumn('postalCade', df.address.pastalCode) \\\n",
        "    .withCalumn('state', df.address.state) \\\n",
        "    .withColumn('streetAddress', df.address.streetAddress) \\\n",
        "    .withColumn('telePhoneNumber ', explode(df.phoneNumber.number)) \\\n",
        "    .withColumn('telePhone Type', explode(df.phoneNumber.type))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e354bb5d",
      "metadata": {
        "id": "e354bb5d"
      },
      "source": [
        "**Updating Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f3a44d",
      "metadata": {
        "id": "f5f3a44d"
      },
      "outputs": [],
      "source": [
        "df = df.withColumnRenamed('telePhoneNumber ', 'phoneNumber')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2387fdf",
      "metadata": {
        "id": "d2387fdf"
      },
      "source": [
        "**Removing Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07fa62a9",
      "metadata": {
        "id": "07fa62a9"
      },
      "outputs": [],
      "source": [
        "df = df.drop(\"address\", \"phoneNumber\")\n",
        "df = df.drop(df.address).drop(df.phoneNumber)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a3cfe08",
      "metadata": {
        "id": "4a3cfe08"
      },
      "source": [
        "### Missing & Replacing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f166f277",
      "metadata": {
        "id": "f166f277"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.na.fill(50).show()  # Replace null values\n",
        "df.na.drop().shaw()  # Return new df omitting rows with null values\n",
        "df.na \\  # Return new df replacing one value with another\n",
        ".replace(10, 20) \\\n",
        "    .show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb93ab65",
      "metadata": {
        "id": "fb93ab65"
      },
      "source": [
        "\n",
        "### GroupBy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c276381a",
      "metadata": {
        "id": "c276381a"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.groupBy(\"age\")\\  # Group by age, count the members in the groups\n",
        ".count() \\\n",
        "    .show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb9f5bc",
      "metadata": {
        "id": "8bb9f5bc"
      },
      "source": [
        "\n",
        "### Sort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bee3535",
      "metadata": {
        "id": "1bee3535"
      },
      "outputs": [],
      "source": [
        "\n",
        "peopledf.sort(peopledf.age.desc()).collect()\n",
        "df.sort(\"age\", ascending=False).collect()\n",
        "df.orderBy([\"age\", \"city\"], ascending=[0, 1])\\\n",
        "    .collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d4950b",
      "metadata": {
        "id": "20d4950b"
      },
      "source": [
        "\n",
        "### Repartitioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58804a79",
      "metadata": {
        "id": "58804a79"
      },
      "outputs": [],
      "source": [
        "\n",
        "   df.repartitian(10)\\  # df with 10 partitions\n",
        "      .rdd \\\n",
        "           .getNumPartitions()\n",
        "    df.coalesce(1).rdd.getNumPartitions()  # df with 1 partition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6794e384",
      "metadata": {
        "id": "6794e384"
      },
      "source": [
        "\n",
        "### Running Queries Programmatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64afde8",
      "metadata": {
        "id": "e64afde8"
      },
      "outputs": [],
      "source": [
        "peopledf.createGlobalTempView(\"people\")\n",
        "df.createTempView(\"customer\")\n",
        "df.createOrReplaceTempView(\"customer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c380223a",
      "metadata": {
        "id": "c380223a"
      },
      "source": [
        "**Query Views**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0061571d",
      "metadata": {
        "id": "0061571d"
      },
      "outputs": [],
      "source": [
        "\n",
        "df5 = spark.sql(\"SELECT * FROM customer\").show()\n",
        "peopledf2 = spark.sql(\"SELEC T* FROM global_ temp.people\")\\\n",
        "    .show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6bbaa7",
      "metadata": {
        "id": "3e6bbaa7"
      },
      "source": [
        "\n",
        "### **Inspect Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e145773c",
      "metadata": {
        "id": "e145773c"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.dtypes  # Return df column names and data types\n",
        "df.show()  # Display the content of df\n",
        "df.head()  # Return first n raws\n",
        "df.first()  # Return first row\n",
        "df.take(2)  # Return the first n rows\n",
        "df.schema Return the schema of df\n",
        "df.describe().show()  # Compute summary statistics\n",
        "df.columns Return the columns of df\n",
        "df.count()  # Count the number of rows in df\n",
        "df.distinct().count()  # Count the number of distinct rows in df\n",
        "df.printSchema()  # Print the schema of df\n",
        "df.explain()  # Print the (logical and physical) plans\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d350618",
      "metadata": {
        "id": "7d350618"
      },
      "source": [
        "\n",
        "### Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78dce1f2",
      "metadata": {
        "id": "78dce1f2"
      },
      "outputs": [],
      "source": [
        "**Data Structures**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63767fc8",
      "metadata": {
        "id": "63767fc8"
      },
      "outputs": [],
      "source": [
        "rddl = df.rdd  # Convert df into an ROD\n",
        "df.taJSON().first()  # Convert df into a ROD of string\n",
        "df.toPandas()  # Return the contents of df as Pandas DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8cb8fc0",
      "metadata": {
        "id": "a8cb8fc0"
      },
      "source": [
        "**Write** & **Save to Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7535f5ac",
      "metadata": {
        "id": "7535f5ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.select(\"firstName\", \"city\")\\\n",
        "    .write \\\n",
        "    .save(\"nameAndCity.parquet\")\n",
        "df.select(\"firstName\", \"age\") \\\n",
        "    .write \\\n",
        "    .save(\"namesAndAges.json\", format=\"json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec6ab9f",
      "metadata": {
        "id": "7ec6ab9f"
      },
      "source": [
        "\n",
        "### **Stopping SparkSession**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e96ee86",
      "metadata": {
        "id": "3e96ee86"
      },
      "outputs": [],
      "source": [
        ">> spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92af1dd2",
      "metadata": {
        "id": "92af1dd2"
      },
      "source": [
        "## **PySpark RDD**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5488cfd3",
      "metadata": {
        "id": "5488cfd3"
      },
      "source": [
        "PySpark is the Spark Python API that exposes the Spark programming model\n",
        "to Python."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8cea036",
      "metadata": {
        "id": "f8cea036"
      },
      "source": [
        "**Inspect SparkContext**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e37ba0",
      "metadata": {
        "id": "73e37ba0"
      },
      "outputs": [],
      "source": [
        "\n",
        "sc.version  # Retrieve SparkContext version\n",
        "sc.pythonVer  # Retrieve Python version\n",
        "sc.master  # Master URL to connect to\n",
        "str(sc.sparkHome)  # Path where Spark is installed an worker nodes\n",
        "str(sc.sparkUser())  # Retrieve name of the Spark User running SparkContext\n",
        "sc.appName  # Return application name\n",
        "sc.applicationld  # Retrieve application ID\n",
        "sc.defaultParallelism  # Return default level of parallelism\n",
        "sc.defaultMinPartitions  # Default minimum number of partitions for RDDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf3c9ce",
      "metadata": {
        "id": "acf3c9ce"
      },
      "source": [
        "\n",
        "**Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42eab341",
      "metadata": {
        "id": "42eab341"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "conf = (SparkConf()\n",
        "        .setMaster(\"local\")\n",
        "        .setAppName(\"My app\")\n",
        "        .set(\"spark.executor.memory\", \"1g\"))\n",
        "sc = SparkContext(conf=conf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90dc3678",
      "metadata": {
        "id": "90dc3678"
      },
      "source": [
        "**Using The Shell**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29001d37",
      "metadata": {
        "id": "29001d37"
      },
      "source": [
        "In the PySpark shell, a special interpreter aware SparkContext is\n",
        "already created in the variable called sc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f535387d",
      "metadata": {
        "id": "f535387d"
      },
      "outputs": [],
      "source": [
        "$ ./bin/spark shell - -master local[2]\n",
        "$ ./bin/pyspark - -master local[4] - -py files code.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83435805",
      "metadata": {
        "id": "83435805"
      },
      "source": [
        "Set which master the context connects to with the –master argument, and\n",
        "add Python **.zip, .egg** or **.py** files to the runtime path by\n",
        "passing a comma separated list to –py-files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb359b2",
      "metadata": {
        "id": "cfb359b2"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa51f3a",
      "metadata": {
        "id": "3fa51f3a"
      },
      "source": [
        "**Parallelized Collections**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "974b66cf",
      "metadata": {
        "id": "974b66cf"
      },
      "outputs": [],
      "source": [
        "\n",
        " rdd = sc.parallelize([('a', 7), ('a', 2), ('b', 2)])\n",
        " rdd = sc.parallelize([('a', 2), ('d', 1), ('b', 1)])\n",
        " rdd3 = sc.parallelize(range(100))\n",
        " rdd4 = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]),\n",
        "                            (\"b\", [\"p\", \"r\"])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89693a0a",
      "metadata": {
        "id": "89693a0a"
      },
      "source": [
        "\n",
        "**External Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9fc9a5",
      "metadata": {
        "id": "1c9fc9a5"
      },
      "source": [
        "Read either one text file from HDFS.a local file system or or any\n",
        "Hadoop-supported file system URI with textFile(). or read in a directory\n",
        "of text files with wholeTextFiles()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a0e6cf",
      "metadata": {
        "id": "49a0e6cf"
      },
      "outputs": [],
      "source": [
        "\n",
        "textFile = sc.textFile(\"/my/directory/*.txt\")\n",
        "textFile2 = sc.wholeTextFiles(\"/my/directory/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed7be65",
      "metadata": {
        "id": "2ed7be65"
      },
      "source": [
        "\n",
        "### Retrieving RDD Information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28183007",
      "metadata": {
        "id": "28183007"
      },
      "source": [
        "**Basic Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aad5f88",
      "metadata": {
        "id": "0aad5f88"
      },
      "outputs": [],
      "source": [
        "\n",
        " rdd.getNumPartitions()  # List the number of partitions\n",
        " rdd.count()  # Count ROD instances 3\n",
        " rdd.countByKey()  # Count ROD instances by key\n",
        "defaultdict(< type 'int' > , {'a': 2, 'b' : 1})\n",
        " rdd.countByValue()  # Count ROD instances by value\n",
        "defaultdict( < type 'int' > , {('b', 2): 1, '(a', 2): 1, ('a', 7): 1})\n",
        " rdd.collectAsMap()  # Return (key,value) pairs as a dictionary\n",
        "{'a': 2, 'b': 2}\n",
        " rdd3.sum()  # Sum of ROD elements 4950\n",
        " sc.parallelize([]).isEmpty()  # Check whether ROD is empty\n",
        "True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9acb561",
      "metadata": {
        "id": "d9acb561"
      },
      "source": [
        "\n",
        "**Summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c699a45",
      "metadata": {
        "id": "1c699a45"
      },
      "outputs": [],
      "source": [
        "\n",
        " rdd3.max()  # Maximum value of ROD elements 99\n",
        " rdd3.min()  # Minimum value of ROD elements\n",
        "0\n",
        " rdd3.mean()  # Mean value of ROD elements\n",
        ", 9.5\n",
        " rdd3.stdev()  # Standard deviation of ROD elements 2a.8660700s772211a\n",
        " rdd3.variance()  # Compute variance of ROD elements 833.25\n",
        " rdd3.histogram(3)  # Compute histogram by bins\n",
        "([0, 33, 66, 991, [33, 33, 3, ])\n",
        " rdd3.stats()  # Summary statistics (count, mean, stdev, max & min)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61520067",
      "metadata": {
        "id": "61520067"
      },
      "source": [
        "\n",
        "### Applying Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aef158c",
      "metadata": {
        "id": "3aef158c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply a function to each ROD element\n",
        "rdd.map(lambda x: x+(x[l], x[0])).callect()\n",
        "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]\n",
        "# Apply a function to each ROD element and flatten the result\n",
        "rdd5 = rdd.flatMap(lambda x: x+(x[l], x[0]))\n",
        "rdd5.collect()\n",
        "['a', 7, 7'a', 'a', 2, 2'a', 'b', 2, 2'b']\n",
        "# Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys\n",
        "rdds.flatMapValues(lambda x: x).callect()\n",
        "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7d9fd93",
      "metadata": {
        "id": "a7d9fd93"
      },
      "source": [
        "\n",
        "### Selecting Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ca176b",
      "metadata": {
        "id": "18ca176b"
      },
      "source": [
        "**Getting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb00560",
      "metadata": {
        "id": "2cb00560"
      },
      "outputs": [],
      "source": [
        "\n",
        "rdd.collect()  # Return a list with all ROD elements\n",
        "[('a', 7), ('a', 2), ('b', 2)]\n",
        "rdd.take(2)  # Take first 2 ROD elements\n",
        "[('a', 7), ('a', 2)]\n",
        "rdd.first()  # Toke first ROD element\n",
        "[('a', 7), ('a', 2)]\n",
        "rdd.top(2)  # Take top 2 ROD elements\n",
        "[('b', 2), ('a', 7)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3794d081",
      "metadata": {
        "id": "3794d081"
      },
      "source": [
        "\n",
        "**Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65406fc",
      "metadata": {
        "id": "d65406fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "rdd3.sample(False, 0.15, 81).collect()  # Return sampled subset of rdd3\n",
        "[3, 4, 27, 31, 40, 41, 42, 43, 60, 76, 79, 80, 86, 97]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87a3d641",
      "metadata": {
        "id": "87a3d641"
      },
      "source": [
        "\n",
        "**Filtering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ded169",
      "metadata": {
        "id": "68ded169"
      },
      "outputs": [],
      "source": [
        "\n",
        "rdd.filter(lambda x: \"a\" in x).collect()  # Filter the ROD\n",
        "[('a', 7), ('a', 2)]\n",
        "rdd5.distinct().callect()  # Return distinct ROD values\n",
        "['a', 2, 'b', 7]\n",
        "rdd.keys().collect()  # Return (key,value) RDD's keys\n",
        "['a', 'a', 'b']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8910dafd",
      "metadata": {
        "id": "8910dafd"
      },
      "outputs": [],
      "source": [
        "def g(x): print(x)\n",
        "\n",
        "\n",
        "rdd.foreach(g)  # Apply a function to all ROD elements\n",
        "\n",
        "('a', 7)\n",
        "('b', 2)\n",
        "('a', 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e2b1616",
      "metadata": {
        "id": "1e2b1616"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Reshaping Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85b3b4f",
      "metadata": {
        "id": "a85b3b4f"
      },
      "source": [
        "**Reducing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe207f8",
      "metadata": {
        "id": "0fe207f8"
      },
      "outputs": [],
      "source": [
        "     # Merge the rdd values for each key\n",
        "     rdd.reduceByKey(lambda x, y: x+y).callect()\n",
        "     [('a', 9), ('b', 2)]\n",
        "     rdd.reduce(lambda a, b: a + b)  # Merge the rdd values\n",
        "    ('a',7,'a',2,'b',2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3541a1b3",
      "metadata": {
        "id": "3541a1b3"
      },
      "source": [
        "**Grouping by**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a13d5a",
      "metadata": {
        "id": "74a13d5a"
      },
      "outputs": [],
      "source": [
        "\n",
        "rdd3.groupBy(lambda x: x % 2)\n",
        ".mapValues(list)\n",
        ".collect()\n",
        "rdd.groupByKey()\n",
        ".mapValues(list)\n",
        ".collect()\n",
        "[('a', [7, 2]), ('b', [2])]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55428481",
      "metadata": {
        "id": "55428481"
      },
      "source": [
        "\n",
        "**Aggregating**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b88f352",
      "metadata": {
        "id": "0b88f352"
      },
      "outputs": [],
      "source": [
        "\n",
        "seqOp = (lambda x, y: (x[0]+y, x[1]+1))\n",
        "combOp = (lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
        "# Aggregate RDD elements of each partition and then the results\n",
        "rdd3.aggregate((0, 0), seqOp, combOp)\n",
        "(4950, 100)\n",
        "# Aggregate values of each RDD key\n",
        "rdd.aggregateByKey((0, 0), seqop, combop).collect()\n",
        "[('a', (9, 2)), ('b', (2, 1))]\n",
        "# Aggregate the elements of each partition, and then the results\n",
        "rdd3.fold(0, add)\n",
        "4950\n",
        "# Merge the values for each key\n",
        "rdd.foldByKey(0, add).collect()\n",
        "[('a', 9), ('b', 2)]\n",
        "# Create tuples of RDD elements by applying a function\n",
        "rdd3.keyBy(lambda x: x+x).collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a69cfb",
      "metadata": {
        "id": "39a69cfb"
      },
      "source": [
        "\n",
        "### Mathematical Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be046320",
      "metadata": {
        "id": "be046320"
      },
      "outputs": [],
      "source": [
        "     # Return each rdd value not contained in rdd2\n",
        "     rdd.subtract(rdd2).collect()\n",
        "    [('b',2),('a',7)]\n",
        "    # Return each (key,value) pair of rdd2 with no matching key in rdd\n",
        "     rdd2.subtractByKey(rdd).collect()\n",
        "    [('d',1)]\n",
        "     rdd.cartesian(rdd2).callect() #Return the Cartesian product of rdd and rdd2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c8aa43",
      "metadata": {
        "id": "69c8aa43"
      },
      "source": [
        "**Sort**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087afe8c",
      "metadata": {
        "id": "087afe8c"
      },
      "outputs": [],
      "source": [
        "     rdd2.sortBy(lambda x: x[l]).collect()  # Sort ROD by given function\n",
        "    [('d',1),('b',1),('a',2)]\n",
        "     rdd2.sartByKey().collect() #Sort (key, value) ROD by key\n",
        "    [('a',2),('b',1),('d',1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c8b9a4d",
      "metadata": {
        "id": "4c8b9a4d"
      },
      "source": [
        "**Repartitioning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3aa8415",
      "metadata": {
        "id": "e3aa8415"
      },
      "outputs": [],
      "source": [
        "\n",
        "rdd.repartitian(4)  # New ROD with 4 partitions\n",
        "rdd.caalesce(1)  # Decrease the number of partitions in the ROD to 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9488ed0",
      "metadata": {
        "id": "c9488ed0"
      },
      "source": [
        "\n",
        "**Saving**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ffae37",
      "metadata": {
        "id": "55ffae37"
      },
      "outputs": [],
      "source": [
        "\n",
        "rdd .saveA sTextFile(\"rdd.txt\")\n",
        "rdd.saveAsHadaapFile(\"hdfs://namenadehost/parent/child\",\n",
        "                     'org.apache.hadoop.mapred.TextOutputFormat')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d51ed0c",
      "metadata": {
        "id": "6d51ed0c"
      },
      "source": [
        "\n",
        "**Execution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b311e0e4",
      "metadata": {
        "id": "b311e0e4"
      },
      "outputs": [],
      "source": [
        "$ ./bin/spark submit examples/src/main/python/pi.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c79e43",
      "metadata": {
        "id": "b3c79e43"
      },
      "source": [
        "**Does PySpark provide a machine learning API?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e2b7d02",
      "metadata": {
        "id": "9e2b7d02"
      },
      "source": [
        "Similar to Spark, PySpark provides a machine learning API which is known\n",
        "as MLlib that supports various ML algorithms like:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d071b77",
      "metadata": {
        "id": "7d071b77"
      },
      "source": [
        "-   mllib.classification − This supports different methods for binary or\n",
        "    multiclass classification and regression analysis like Random\n",
        "    Forest, Decision Tree, Naive Bayes etc.\n",
        "-   mllib.clustering − This is used for solving clustering problems that\n",
        "    aim in grouping entities subsets with one another depending on\n",
        "    similarity.\n",
        "-   mllib.fpm − FPM stands for Frequent Pattern Matching. This library\n",
        "    is used to mine frequent items, subsequences or other structures\n",
        "    that are used for analyzing large datasets.\n",
        "-   mllib.linalg − This is used for solving problems on linear algebra.\n",
        "-   mllib.recommendation − This is used for collaborative filtering and\n",
        "    in recommender systems.\n",
        "-   spark.mllib − This is used for supporting model-based collaborative\n",
        "    filtering where small latent factors are identified using the\n",
        "    Alternating Least Squares (ALS) algorithm which is used for\n",
        "    predicting missing entries.\n",
        "-   mllib.regression − This is used for solving problems using\n",
        "    regression algorithms that find relationships and variable\n",
        "    dependencies.\n",
        "-   **Is PySpark faster than pandas?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ba02c3e",
      "metadata": {
        "id": "9ba02c3e"
      },
      "source": [
        "PySpark supports parallel execution of statements in a distributed\n",
        "environment, i.e on different cores and different machines which are not\n",
        "present in Pandas. This is why PySpark is faster than pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97cc14be",
      "metadata": {
        "id": "97cc14be"
      },
      "source": [
        "**What is PySpark Architecture?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da505aed",
      "metadata": {
        "id": "da505aed"
      },
      "source": [
        "PySpark similar to Apache Spark works in master-slave architecture\n",
        "pattern. Here, the master node is called the Driver and the slave nodes\n",
        "are called the workers. When a Spark application is run, the Spark\n",
        "Driver creates SparkContext which acts as an entry point to the spark\n",
        "application. All the operations are executed on the worker nodes. The\n",
        "resources required for executing the operations on the worker nodes are\n",
        "managed by the Cluster Managers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a630346c",
      "metadata": {
        "id": "a630346c"
      },
      "source": [
        "**What is the common workflow of a spark program?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137cf968",
      "metadata": {
        "id": "137cf968"
      },
      "source": [
        "The most common workflow followed by the spark program is: The first\n",
        "step is to create input RDDs depending on the external data. Data can be\n",
        "obtained from different data sources. Post RDD creation, the RDD\n",
        "transformation operations like filter() or map() are run for creating\n",
        "new RDDs depending on the business logic. If any intermediate RDDs are\n",
        "required to be reused for later purposes, we can persist those RDDs.\n",
        "Lastly, if any action operations like first(), count() etc are present\n",
        "then spark launches it to initiate parallel computation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca984863",
      "metadata": {
        "id": "ca984863"
      },
      "source": [
        "**Congratulations!** You have read an small summary about important\n",
        "things in **Data Science**."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pyspark-in-Google-Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}